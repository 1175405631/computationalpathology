{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlWmfy8s65bN"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28q1CI555p9f"
      },
      "source": [
        "### Connect to Drive and Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "MYfLehyB4TCc",
        "outputId": "332caeca-670e-4776-cbaa-7f5cf676f681"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-27fa79a0-daf9-4b2a-8116-4c93e55b739e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-27fa79a0-daf9-4b2a-8116-4c93e55b739e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"gkyyds\",\"key\":\"194d6831ee94cd778feec959408dc570\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xaWgHajG4USV"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74Kg2r014ZbF",
        "outputId": "f26045cb-fc8c-40c7-d8f6-de6f72c5bb4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyrubtob7Utg"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTeEvG6B7Eqr",
        "outputId": "cd77169f-fb34-450a-e773-732cbad3bfa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openslide-python\n",
            "  Downloading openslide_python-1.4.2-cp311-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from openslide-python) (11.3.0)\n",
            "Downloading openslide_python-1.4.2-cp311-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.whl (36 kB)\n",
            "Installing collected packages: openslide-python\n",
            "Successfully installed openslide-python-1.4.2\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.12/dist-packages (2025.8.28)\n",
            "Collecting zarr\n",
            "  Downloading zarr-3.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting imagecodecs\n",
            "  Downloading imagecodecs-2025.8.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tifffile) (2.0.2)\n",
            "Collecting donfig>=0.8 (from zarr)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting numcodecs>=0.14 (from numcodecs[crc32c]>=0.14->zarr)\n",
            "  Downloading numcodecs-0.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.12/dist-packages (from zarr) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9 in /usr/local/lib/python3.12/dist-packages (from zarr) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from donfig>=0.8->zarr) (6.0.2)\n",
            "Collecting crc32c>=2.7 (from numcodecs[crc32c]>=0.14->zarr)\n",
            "  Downloading crc32c-2.7.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading zarr-3.1.2-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imagecodecs-2025.8.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading numcodecs-0.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crc32c-2.7.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numcodecs, imagecodecs, donfig, crc32c, zarr\n",
            "Successfully installed crc32c-2.7.1 donfig-0.8.1.post1 imagecodecs-2025.8.2 numcodecs-0.16.2 zarr-3.1.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Core WSI handling\n",
        "!pip install openslide-python\n",
        "\n",
        "# # Image processing\n",
        "!pip install opencv-python Pillow\n",
        "\n",
        "!pip install tifffile zarr imagecodecs opencv-python pillow pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XvnHQjwE7iBF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "from glob import glob\n",
        "\n",
        "import tifffile as tiff, zarr\n",
        "import numpy as np\n",
        "import cv2, pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch, torchvision as tv\n",
        "import h5py\n",
        "\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QqOQ8kaG4oB"
      },
      "source": [
        "## Edge Detection - Contour function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TryZzyAXBkLW"
      },
      "outputs": [],
      "source": [
        "def _to_uint8(img):\n",
        "    if img.dtype == np.uint8: return img\n",
        "    x = img.astype(np.float32); mn, mx = float(x.min()), float(x.max())\n",
        "    if mx <= mn: return np.zeros_like(x, dtype=np.uint8)\n",
        "    return ((x - mn) / (mx - mn) * 255.0).astype(np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 通过饱和度分离组织和背景（饱和度高和白色）\n",
        "# 去掉噪点，让组织区域更连贯\n",
        "# 找到轮廓线 cv2.findContours\n",
        "def build_contour_mask_1024(\n",
        "    lowres_rgb, # 低分辨率彩色图\n",
        "    mthresh: int = 41,        # 去噪点\n",
        "    sthresh: int = 8,        # 分离前景背景\n",
        "    close: int = 3,           # morphology closing kernel\n",
        "    min_area_fore: int = 6,  # min area (low-res px) 低分辩像素个数\n",
        "    min_area_hole: int = 4,  # min hole area (low-res px)\n",
        "    max_n_holes: int = 12,     # cap holes per region 最多保留孔洞数量\n",
        "):\n",
        "\n",
        "\n",
        "    img = _to_uint8(lowres_rgb)\n",
        "    # 组织区域更有颜色，和白色对比强\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV); sat = hsv[..., 1]\n",
        "    sat = cv2.medianBlur(sat, int(max(1, mthresh) | 1))\n",
        "    _, bin_s = cv2.threshold(sat, int(sthresh), 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # 填补小缝隙\n",
        "    if close > 0:\n",
        "        kernel = np.ones((int(close), int(close)), np.uint8)\n",
        "        bin_s = cv2.morphologyEx(bin_s, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, hier = cv2.findContours(bin_s, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
        "    if hier is None or len(contours) == 0:\n",
        "        return (bin_s > 0)\n",
        "    hier = hier[0]  # [Next, Prev, First_Child, Parent]\n",
        "    fore_ids = [i for i,h in enumerate(hier) if h[3] == -1]\n",
        "    kept_fore, holes_per_fore = [], []\n",
        "    for fid in fore_ids:\n",
        "        a = cv2.contourArea(contours[fid])\n",
        "        if a <= 0: continue\n",
        "        # collect children (holes)\n",
        "        holes = []\n",
        "        child = hier[fid][2]\n",
        "        while child != -1:\n",
        "            holes.append(child)\n",
        "            child = hier[child][0]\n",
        "        hole_areas = [cv2.contourArea(contours[h]) for h in holes]\n",
        "        real_a = a - (np.sum(hole_areas) if hole_areas else 0.0)\n",
        "        if real_a >= float(min_area_fore):\n",
        "            kept_fore.append(fid)\n",
        "            holes_kept = [h for h in holes if cv2.contourArea(contours[h]) > float(min_area_hole)]\n",
        "            holes_kept = sorted(holes_kept, key=lambda h: cv2.contourArea(contours[h]), reverse=True)[:max_n_holes]\n",
        "            holes_per_fore.append(holes_kept)\n",
        "    H, W = bin_s.shape\n",
        "    mask = np.zeros((H, W), dtype=np.uint8)\n",
        "    if kept_fore:\n",
        "        cv2.drawContours(mask, [contours[i] for i in kept_fore], -1, 255, thickness=cv2.FILLED)\n",
        "    for holes in holes_per_fore:\n",
        "        if holes:\n",
        "            cv2.drawContours(mask, [contours[i] for i in holes], -1, 0, thickness=cv2.FILLED)\n",
        "    return (mask > 0)\n",
        "\n",
        "\n",
        "# 是否留下patch\n",
        "# 组织是否较多？是否有边缘？\n",
        "def patch_keep(mask_bool, x_m, y_m, w_m, h_m, min_tissue = 0.20, min_edge = 0.05):\n",
        "    H, W = mask_bool.shape[:2]\n",
        "    x2, y2 = min(W, x_m + w_m), min(H, y_m + h_m)\n",
        "    if x_m >= x2 or y_m >= y2: return False, {'cov':0.0,'edge':0.0}\n",
        "    win = mask_bool[y_m:y2, x_m:x2]\n",
        "    if win.size == 0: return False, {'cov':0.0,'edge':0.0}\n",
        "    cov = float(win.mean())\n",
        "    if cov == 0.0:\n",
        "        edge_ratio = 0.0\n",
        "    else:\n",
        "        eroded = cv2.erode(win.astype(np.uint8), np.ones((3,3), np.uint8), 1).astype(bool)\n",
        "        border = win ^ eroded\n",
        "        edge_ratio = float(border.mean())\n",
        "    keep = (cov >= min_tissue) or (edge_ratio >= min_edge)\n",
        "    return keep, {'cov': cov, 'edge': edge_ratio}\n",
        "\n",
        "# 打分数\n",
        "# score = 0.6 * 组织覆盖率 + 0.4 * edge\n",
        "def rank_key(stats: dict, alpha: float = 0.6):\n",
        "    return alpha*float(stats.get('cov',0.0)) + (1.0-alpha)*float(stats.get('edge',0.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuKcliGy5wQE"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tMtz_p-A4axj"
      },
      "outputs": [],
      "source": [
        "drive_path = '/content/drive/MyDrive/PANDA_OneImage'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "image_id = '0005f7aaab2800f6170c399693a96917'\n",
        "image_filename = f'train_images/{image_id}.tiff'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ2hyLD94mT_",
        "outputId": "44ea6890-240a-4ecc-eade-50c6b1ab3e27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "0005f7aaab2800f6170c399693a96917.tiff: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c prostate-cancer-grade-assessment -f train.csv -p '{drive_path}'\n",
        "!kaggle competitions download -c prostate-cancer-grade-assessment -f '{image_filename}' -p '{drive_path}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McnWP4JsxYxl",
        "outputId": "907e75e4-5050-445b-8e98-511960dd2415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train.csv', '0005f7aaab2800f6170c399693a96917.tiff', 'extracted', 'patches_10x', 'patches_20x']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir(drive_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6PtuKSjpUN0",
        "outputId": "5d48648a-a81d-461a-d090-3c4fac7f0284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Exists: True\n",
            "Size (MB): 15.38\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, pathlib\n",
        "drive_path = '/content/drive/MyDrive/PANDA_OneImage'\n",
        "image_id = '0005f7aaab2800f6170c399693a96917'\n",
        "image_filename = f'{image_id}.tiff'\n",
        "tiff_path = os.path.join(drive_path, image_filename)\n",
        "\n",
        "print(\"Exists:\", os.path.exists(tiff_path))\n",
        "print(\"Size (MB):\", round(os.path.getsize(tiff_path)/1024/1024, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYMrpHq7zdxa"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY3rYfKO8p1t"
      },
      "source": [
        "### Import and unzip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKY4OoOz8Hok"
      },
      "source": [
        "*   Many documents implment with OpenSlide, but this Kaggle one does not work as it's a zip --> tifffile and zarr\n",
        "\n",
        "\n",
        "*   forgetground threshold: reject patches if less than % of pixels are tissues\n",
        "\n",
        "*   min std: reject patches with low variations (blank)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TuofGemusdP1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# with tiff.TiffFile(f'{drive_path}/0005f7aaab2800f6170c399693a96917.tiff') as tf:\n",
        "#   print(len(tf.series[0].levels))   # number of pyramid levels\n",
        "#   for lvl in tf.series[0].levels:\n",
        "#     print(lvl.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yaVREMyN1bnY"
      },
      "outputs": [],
      "source": [
        "ZIP_OR_TIFF_PATH = f'/content/drive/MyDrive/PANDA_OneImage/{image_id}.tiff'\n",
        "\n",
        "# output path\n",
        "EXTRACT_DIR = '/content/drive/MyDrive/PANDA_OneImage/extracted'     # where we unzip and get the real tiff\n",
        "OUT_DIR     = '/content/drive/MyDrive/PANDA_OneImage/patches_20x'   # where we save patches/CSV, all outputs\n",
        "\n",
        "\n",
        "\n",
        "# resolution flag\n",
        "# reso = '20x'\n",
        "reso = '20x'\n",
        "\n",
        "Path(EXTRACT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bsiMhCLq14kF"
      },
      "outputs": [],
      "source": [
        "# unzip the file and return the real tiff\n",
        "def get_real_tiff(path: str):\n",
        "    # if file starts with 'PK', it's a ZIP\n",
        "    with open(path, 'rb') as f:\n",
        "        sig = f.read(2)\n",
        "    if sig == b'PK':\n",
        "        with zipfile.ZipFile(path) as z:\n",
        "            print('Archive contents:', z.namelist())\n",
        "            z.extractall(EXTRACT_DIR)\n",
        "        tiffs = sorted(glob(os.path.join(EXTRACT_DIR, '**', '*.tif*'), recursive=True))\n",
        "        if not tiffs:\n",
        "            raise FileNotFoundError('No .tif/.tiff found after extraction.')\n",
        "        return tiffs[0]\n",
        "    return path\n",
        "\n",
        "\n",
        "# zarr: n-dimensional array, like NumPy, but load what you need when you need\n",
        "# WSIs giant -> zarr\n",
        "# get the actual array of the pixel data\n",
        "def _to_zarr_array(znode):\n",
        "    obj = zarr.open(znode, mode='r')\n",
        "    # if a single array\n",
        "    if isinstance(obj, zarr.Array):\n",
        "        return obj\n",
        "    # if a group with multi arrays\n",
        "    if isinstance(obj, zarr.Group):\n",
        "        keys = list(obj.array_keys())\n",
        "        if not keys:\n",
        "            raise ValueError('Zarr Group has no arrays.')\n",
        "        return obj[keys[0]]\n",
        "    raise TypeError(f'Unexpected zarr node type: {type(obj)}')\n",
        "\n",
        "\n",
        "# open the first image in tiff\n",
        "# collect all levels （pyramid) , 40x --> 20x --> 10x....\n",
        "def open_pyramid_as_zarr(tf: tiff.TiffFile):\n",
        "\n",
        "    s0 = tf.series[0]\n",
        "    arr0 = _to_zarr_array(s0.aszarr())        # level 0 (highest resolution)\n",
        "    levels = [arr0] + [_to_zarr_array(l.aszarr()) for l in s0.levels]\n",
        "    # compute downsample factors related to level 0\n",
        "    # eg: L0 wid = 1000, L1 wid = 500, 1000 / 500 = 2\n",
        "    downs = [1.0] + [arr0.shape[-2] / lvl.shape[-2] for lvl in levels[1:]]\n",
        "    return levels, downs\n",
        "\n",
        "# pick the one closest to my target, 20X here\n",
        "def pick_level_for_target(downs, target_down=1.0):\n",
        "    return int(np.argmin([abs(d - target_down) for d in downs]))\n",
        "\n",
        "# standardize all to RGB unit8 format (H, W, 3)\n",
        "def ensure_hwc(tile: np.ndarray):\n",
        "    t = tile\n",
        "    if t.ndim == 2: # grayscale\n",
        "        t = np.stack([t]*3, axis=-1)\n",
        "    elif t.ndim == 3 and t.shape[0] in (3,4) and t.shape[-1] not in (3,4): # (C, H, W)\n",
        "        t = np.moveaxis(t, 0, -1)  # (C,H,W) -> (H,W,C)\n",
        "    if t.shape[-1] > 3: # RGBA with alpha\n",
        "        t = t[..., :3]            # drop alpha\n",
        "    if t.dtype != np.uint8: # other format\n",
        "        # best-effort clamp/convert (many WSIs are already uint8)\n",
        "        t = np.clip(t, 0, 255).astype(np.uint8)\n",
        "    return t\n",
        "\n",
        "def save_png(arr: np.ndarray, path: Path):\n",
        "    Image.fromarray(arr).save(path, format='PNG', compress_level=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcf5CDNR16Ns",
        "outputId": "8443284e-6255-4563-e730-f3512877aaeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive contents: ['0005f7aaab2800f6170c399693a96917.tiff']\n",
            "Using TIFF: /content/drive/MyDrive/PANDA_OneImage/extracted/0005f7aaab2800f6170c399693a96917.tiff\n"
          ]
        }
      ],
      "source": [
        "REAL_TIFF_PATH = get_real_tiff(ZIP_OR_TIFF_PATH)\n",
        "print('Using TIFF:', REAL_TIFF_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xm8h6hrxqB76"
      },
      "outputs": [],
      "source": [
        "with tiff.TiffFile(REAL_TIFF_PATH) as tf:\n",
        "    series = tf.series[0]\n",
        "    low = series.levels[-1].asarray()\n",
        "    img = Image.fromarray(low)\n",
        "\n",
        "img.save(f\"original{image_id}.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5xjN856tTWW",
        "outputId": "9be9438a-efd5-423e-d83d-6bd9c21f4e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "(29440, 27648, 3)\n",
            "(7360, 6912, 3)\n",
            "(1840, 1728, 3)\n"
          ]
        }
      ],
      "source": [
        "import tifffile as tiff\n",
        "with tiff.TiffFile('/content/drive/MyDrive/PANDA_OneImage/extracted/0005f7aaab2800f6170c399693a96917.tiff') as tf:\n",
        "    print(len(tf.series[0].levels))   # number of pyramid levels\n",
        "    for lvl in tf.series[0].levels:\n",
        "        print(lvl.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s7fT98GZDD5j"
      },
      "outputs": [],
      "source": [
        "if reso == '20x':\n",
        "  # base level: 20X\n",
        "  hi_level = 0\n",
        "  PATCH, STRIDE = 1024, 512\n",
        "  TARGET_DOWN = 1.0\n",
        "\n",
        "else:\n",
        "  # 10x level\n",
        "  hi_level = 1\n",
        "  PATCH, STRIDE = 512, 256\n",
        "  TARGET_DOWN = 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEGwdPnkt9Nw",
        "outputId": "6b118c4e-5809-4cc6-e923-07cc4fb97ac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pyramid downsample factors vs level-0: ['1.00×', '1.00×', '4.00×', '16.00×']\n",
            "Chosen level: 0  (downsample 1.00×),  shape≈(29440, 27648, …)\n"
          ]
        }
      ],
      "source": [
        "with tiff.TiffFile(REAL_TIFF_PATH) as tf:\n",
        "    levels, downs = open_pyramid_as_zarr(tf)\n",
        "\n",
        "print('Pyramid downsample factors vs level-0:', [f'{d:.2f}×' for d in downs])\n",
        "\n",
        "L = pick_level_for_target(downs, TARGET_DOWN)\n",
        "arrL = levels[L]\n",
        "\n",
        "# extract H and W\n",
        "H, W = arrL.shape[-3], arrL.shape[-2]\n",
        "print(f'Chosen level: {L}  (downsample {downs[L]:.2f}×),  shape≈({H}, {W}, …)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fbBewaLG83n"
      },
      "source": [
        "## Run Contour part to get the non-black part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehxgpwYqg-TJ",
        "outputId": "736d7767-0583-433a-d4da-b506b8b8102a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2357797697.py:44: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  Image.fromarray(_to_uint8(tile), 'RGB').save(os.path.join(out_dir, f'p_x{x}_y{y}_s{score:.3f}.png'))\n"
          ]
        }
      ],
      "source": [
        "# choose the lowest reso to build the mask\n",
        "low_level = len(levels) - 1\n",
        "arr_low = levels[low_level]\n",
        "lowres_rgb = np.asarray(arr_low)  # (H_low, W_low, 3) uint8\n",
        "H_low, W_low = lowres_rgb.shape[:2]\n",
        "\n",
        "\n",
        "\n",
        "arr0 = levels[hi_level]\n",
        "H0, W0 = arr0.shape[0], arr0.shape[1]\n",
        "\n",
        "# Map factor: pixels at hi_level → pixels at low_level\n",
        "# If your 'downs' is defined as (down from level 0), then:\n",
        "#   scale_hi_to_low = downs[hi_level] / downs[low_level]\n",
        "# For hi_level=0 this simplifies to:\n",
        "scale_hi_to_low = downs[hi_level] / downs[low_level]\n",
        "\n",
        "# Build the contour mask at the low-res level\n",
        "contour_mask = build_contour_mask_1024(lowres_rgb)\n",
        "\n",
        "\n",
        "out_dir = f'patches_out_{reso}'; os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "kept = []\n",
        "for y in range(0, H0 - PATCH + 1, STRIDE):\n",
        "    for x in range(0, W0 - PATCH + 1, STRIDE):\n",
        "        # Map hi-res patch → low-res mask window\n",
        "        x_m = int(x * scale_hi_to_low)\n",
        "        y_m = int(y * scale_hi_to_low)\n",
        "        w_m = max(1, int(PATCH * scale_hi_to_low))\n",
        "        h_m = max(1, int(PATCH * scale_hi_to_low))\n",
        "\n",
        "        keep, stats = patch_keep(contour_mask, x_m, y_m, w_m, h_m, min_tissue=0.15, min_edge=0.04)\n",
        "        if not keep:\n",
        "            continue\n",
        "\n",
        "        score = rank_key(stats, alpha=0.6)\n",
        "        kept.append((score, x, y))\n",
        "\n",
        "        # Read hi-res tile directly from zarr and save\n",
        "        tile = np.asarray(arr0[y:y+PATCH, x:x+PATCH, :])\n",
        "        if tile.shape[:2] != (PATCH, PATCH):\n",
        "            continue\n",
        "        Image.fromarray(_to_uint8(tile), 'RGB').save(os.path.join(out_dir, f'p_x{x}_y{y}_s{score:.3f}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmwRFC1cD-kV",
        "outputId": "e9194124-8486-4b5d-ccf0-de8975f46243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstructed 20x saved at reconstructed_20x.png\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Folder where your patches are stored\n",
        "PATCH_DIR = f\"patches_out_{reso}\"\n",
        "\n",
        "# Regex to parse filenames like p_x1024_y1024_s0.187.png\n",
        "pattern = re.compile(r\"p_x(\\d+)_y(\\d+)_s[\\d.]+\\.png\")\n",
        "\n",
        "# Load all patches and record their coordinates\n",
        "patches = []\n",
        "max_x, max_y = 0, 0\n",
        "\n",
        "for fname in os.listdir(PATCH_DIR):\n",
        "    match = pattern.match(fname)\n",
        "    if not match:\n",
        "        continue\n",
        "    x, y = int(match.group(1)), int(match.group(2))\n",
        "    img = Image.open(os.path.join(PATCH_DIR, fname))\n",
        "    w, h = img.size\n",
        "    patches.append((x, y, img))\n",
        "\n",
        "    # Track max extent of the canvas\n",
        "    max_x = max(max_x, x + w)\n",
        "    max_y = max(max_y, y + h)\n",
        "\n",
        "# Create a blank canvas large enough to hold all patches\n",
        "canvas = Image.new(\"RGB\", (max_x, max_y), (255, 255, 255))\n",
        "\n",
        "# Paste each patch onto the canvas\n",
        "for x, y, img in patches:\n",
        "    canvas.paste(img, (x, y))\n",
        "\n",
        "# Save the reconstructed image\n",
        "out_path = f\"reconstructed_{reso}.png\"\n",
        "canvas.save(out_path)\n",
        "print(f\"Reconstructed {reso} saved at {out_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Model SetUp\n",
        "\n"
      ],
      "metadata": {
        "id": "V1C5wLwQcN-N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dcCve15FcNon"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VNyTi9MUicqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d72d6e-5eb6-495b-acb5-2dca21ca7745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install open_clip_torch tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, math, csv\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import open_clip  # open_clip_torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "hsfnPQXFcJ99",
        "outputId": "94715f9c-cf12-4b84-8803-1540877d5cfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 你的 patch 目录（10x 或 20x）\n",
        "PATCH_DIR = \"patches_out_20x\"   # 例如：patches_out_10x / patches_out_20x\n",
        "\n",
        "# 目录分辨率（自动从目录名猜，不放心可手填）\n",
        "FROM_RESO = \"10x\" if \"10x\" in PATCH_DIR.lower() else (\"20x\" if \"20x\" in PATCH_DIR.lower() else \"10x\")\n",
        "\n",
        "# 目标坐标系：统一回帖到 20x（如需 10x 自己改）\n",
        "TARGET_RESO = \"20x\"\n",
        "\n",
        "# 10x→20x 的缩放比例（通常≈2.0，亦可从TIFF金字塔精确计算）\n",
        "SCALE_10X_TO_20X = 2.0\n",
        "\n",
        "# patch 尺寸（像素）\n",
        "PATCH_SIZE_10X = 512\n",
        "PATCH_SIZE_20X = 1024\n",
        "\n",
        "# （可选）送模型前统一 resize 到固定尺寸；None 表示使用 open_clip 自带的 preprocess\n",
        "RESIZE_MODEL_INPUT = None  # 比如 224；None 表示不用\n",
        "\n",
        "# 零样本文本标签（可按需修改/增删，建议英文专业术语更稳）\n",
        "LABELS = [\n",
        "    \"benign gland\",\n",
        "    \"Gleason pattern 3\",\n",
        "    \"Gleason pattern 4\",\n",
        "    \"Gleason pattern 5\",\n",
        "    \"stroma\"\n",
        "]\n",
        "\n",
        "# 输出目录前缀\n",
        "OUT_PREFIX = \"out_conch_style\"  # 会生成 .npy/.csv/.png 等文件\n",
        "os.makedirs(OUT_PREFIX, exist_ok=True)\n",
        "\n",
        "print(\"PATCH_DIR =\", PATCH_DIR)\n",
        "print(\"FROM_RESO =\", FROM_RESO)\n"
      ],
      "metadata": {
        "id": "d71w3LYccSbB",
        "outputId": "15c925a9-3a91-4613-a0d2-2f8e81a2957c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PATCH_DIR = patches_out_20x\n",
            "FROM_RESO = 20x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 解析文件名：p_x1024_y1280_s0.389.png\n",
        "FNAME_RE = re.compile(r\"p_x(\\d+)_y(\\d+)_s([\\d.]+)\\.png\", re.IGNORECASE)\n",
        "\n",
        "def parse_patch_filename(fn: str) -> Tuple[int, int, float]:\n",
        "    m = FNAME_RE.search(os.path.basename(fn))\n",
        "    if not m:\n",
        "        raise ValueError(f\"Bad patch filename: {fn}\")\n",
        "    x = int(m.group(1)); y = int(m.group(2)); score = float(m.group(3))\n",
        "    return x, y, score\n",
        "\n",
        "def map_coords_to_20x(x: int, y: int,\n",
        "                      from_reso: str,\n",
        "                      scale_10x_to_20x: float = 2.0) -> Tuple[int, int]:\n",
        "    \"\"\"把 10x/20x 的坐标映射到 20x 坐标系。\"\"\"\n",
        "    if from_reso.lower() == \"20x\":\n",
        "        return x, y\n",
        "    elif from_reso.lower() == \"10x\":\n",
        "        return int(round(x * scale_10x_to_20x)), int(round(y * scale_10x_to_20x))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported from_reso = {from_reso}\")\n",
        "\n",
        "def patch_size_in_20x(from_reso: str,\n",
        "                      patch_size_10x: int,\n",
        "                      patch_size_20x: int,\n",
        "                      scale_10x_to_20x: float = 2.0) -> int:\n",
        "    \"\"\"返回该分辨率的 patch 在 20x 坐标系下的尺寸（像素）。\"\"\"\n",
        "    if from_reso.lower() == \"20x\":\n",
        "        return patch_size_20x\n",
        "    elif from_reso.lower() == \"10x\":\n",
        "        return int(round(patch_size_10x * scale_10x_to_20x))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported from_reso = {from_reso}\")\n",
        "\n",
        "def build_patch_index(patch_dir: str) -> List[Dict]:\n",
        "    files = sorted([str(p) for p in Path(patch_dir).glob(\"p_x*_y*_s*.png\")])\n",
        "    items = []\n",
        "    for fn in files:\n",
        "        x, y, s = parse_patch_filename(fn)\n",
        "        items.append({\"path\": fn, \"x\": x, \"y\": y, \"score\": s})\n",
        "    print(f\"Found {len(items)} patches.\")\n",
        "    return items\n",
        "\n",
        "def make_canvas_size(items: List[Dict]) -> Tuple[int, int]:\n",
        "    W = 0; H = 0\n",
        "    for it in items:\n",
        "        W = max(W, it[\"x20\"] + it[\"ps20\"])\n",
        "        H = max(H, it[\"y20\"] + it[\"ps20\"])\n",
        "    return W, H\n"
      ],
      "metadata": {
        "id": "WFwJ0GyXfizp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ZSModel:\n",
        "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        self.device = device\n",
        "        model_name, pretrained = \"ViT-B-32\", \"openai\"  # 也可换 ViT-L-14 等\n",
        "        # 返回: model, preprocess_train, preprocess_val\n",
        "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n",
        "            model_name, pretrained=pretrained\n",
        "        )\n",
        "        self.model.to(self.device).eval()\n",
        "        self.tokenizer = open_clip.get_tokenizer(model_name)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_image(self, pil_img: Image.Image) -> np.ndarray:\n",
        "        # 如果用 CONCH：在这换 conch_preprocess(img)+model.encode_image(...)\n",
        "        if RESIZE_MODEL_INPUT is not None:\n",
        "            pil_img = pil_img.resize((RESIZE_MODEL_INPUT, RESIZE_MODEL_INPUT), Image.BILINEAR)\n",
        "        img = self.preprocess(pil_img).unsqueeze(0).to(self.device)\n",
        "        feats = self.model.encode_image(img)  # (1, D)\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "        return feats.squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_texts(self, labels: List[str]) -> np.ndarray:\n",
        "        # 如果用 CONCH：在这换相应文本编码接口\n",
        "        toks = self.tokenizer(labels).to(self.device)\n",
        "        feats = self.model.encode_text(toks)  # (K, D)\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "        return feats.detach().cpu().numpy()\n",
        "\n",
        "zs = ZSModel(device)\n",
        "print(\"Model ready.\")\n"
      ],
      "metadata": {
        "id": "KNutgbsTfxbw",
        "outputId": "4268edda-0dbf-4123-d3ec-6804243825c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "10c29447e5b54c9f94c5bac7e385c4d0",
            "2693390143be4362a823cb9a6779137a",
            "5e8a2fda75e44264bd9b87307c0a1214",
            "adc9e5211bde4ee386d2351b5ba8d364",
            "f0d2f158e885474c9d414edf8fc25ee4",
            "d7cf18eaa92f4eaa902c95c79a1a1c26",
            "44b88f14272141f284f5ac1906de8d29",
            "26040edd1b2b4bd4a2f9e463ad47a767",
            "7d8bf235dca3435b8c55749503f66146",
            "b3615fd32a65472ba0091dd4304f8a93",
            "49682ea8502f4b10ae7b64be4fff3f44"
          ]
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10c29447e5b54c9f94c5bac7e385c4d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# --- 保存为 .npz（推荐），避免 pickle ---\n",
        "NPZ_PATH = os.path.join(OUT_PREFIX, \"patch_embeddings.npz\")\n",
        "\n",
        "np.savez_compressed(\n",
        "    NPZ_PATH,\n",
        "    paths=np.array(paths, dtype='U'),  # 字符串数组\n",
        "    x20=x20,\n",
        "    y20=y20,\n",
        "    ps20=ps20a,\n",
        "    score=score,\n",
        "    embs=embs,  # (N, D) float32\n",
        "    meta=np.array([json.dumps({\n",
        "        \"from_reso\": FROM_RESO,\n",
        "        \"target_reso\": TARGET_RESO,\n",
        "        \"scale_10x_to_20x\": SCALE_10X_TO_20X,\n",
        "        \"patch_size_10x\": PATCH_SIZE_10X,\n",
        "        \"patch_size_20x\": PATCH_SIZE_20X,\n",
        "        \"labels\": LABELS,\n",
        "        \"model\": \"open_clip ViT-B/32 openai\",\n",
        "        \"resize_model_input\": RESIZE_MODEL_INPUT\n",
        "    })], dtype='U')  # 放到数组里，保持 npz 兼容\n",
        ")\n",
        "\n",
        "print(\"Saved:\", NPZ_PATH, \"embs shape:\", embs.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "rH6B468dfz8P",
        "outputId": "875efb48-c40b-4b30-c42c-9bd2d92f618a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: out_conch_style/patch_embeddings.npz embs shape: (166, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 从 .npz 载入（可跨重启） ===\n",
        "import json, csv\n",
        "\n",
        "NPZ_PATH = os.path.join(OUT_PREFIX, \"patch_embeddings.npz\")\n",
        "data = np.load(NPZ_PATH, allow_pickle=False)\n",
        "\n",
        "paths = data[\"paths\"]\n",
        "x20   = data[\"x20\"]\n",
        "y20   = data[\"y20\"]\n",
        "ps20a = data[\"ps20\"]\n",
        "score = data[\"score\"]\n",
        "embs  = data[\"embs\"]          # (N, D) float32\n",
        "meta  = json.loads(str(data[\"meta\"][0]))\n",
        "\n",
        "# 如果你重启过内核，确保 zs/LABELS 等还在；没在就再跑下 Cell 4/Cell 2\n",
        "print(\"Loaded:\", NPZ_PATH, \"| N =\", len(paths), \"| embs shape:\", embs.shape)\n",
        "\n",
        "# === 还原 items（供热图/可视化使用，不把 emb 塞进去以节省内存） ===\n",
        "items = []\n",
        "for i in range(len(paths)):\n",
        "    items.append({\n",
        "        \"path\": str(paths[i]),\n",
        "        \"x20\": int(x20[i]),\n",
        "        \"y20\": int(y20[i]),\n",
        "        \"ps20\": int(ps20a[i]),\n",
        "        \"score\": float(score[i]),\n",
        "    })\n",
        "\n",
        "# === 文本零样本编码 ===\n",
        "# 需要 zs（Cell 4 里已构建 ZSModel）；如果未定义，请先跑 Cell 4\n",
        "text_embs = zs.encode_texts(LABELS)  # (K, D)\n",
        "\n",
        "# === 计算余弦相似度（emb 已 L2 归一化，点积即相似度） ===\n",
        "sims = embs @ text_embs.T            # (N, K)\n",
        "pred_idx = sims.argmax(axis=1)       # (N,)\n",
        "pred_label = [LABELS[i] for i in pred_idx]\n",
        "pred_score = sims.max(axis=1)\n",
        "\n",
        "# === 导出 CSV ===\n",
        "CSV_PATH = os.path.join(OUT_PREFIX, \"patch_predictions.csv\")\n",
        "header = [\"path\", \"x20\", \"y20\", \"ps20\", \"score\", \"pred_label\", \"pred_score\"] + [f\"sim_{lbl}\" for lbl in LABELS]\n",
        "with open(CSV_PATH, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow(header)\n",
        "    for i in range(len(items)):\n",
        "        row = [\n",
        "            items[i][\"path\"], items[i][\"x20\"], items[i][\"y20\"], items[i][\"ps20\"],\n",
        "            float(items[i][\"score\"]), pred_label[i], float(pred_score[i])\n",
        "        ]\n",
        "        row += [float(sims[i, j]) for j in range(len(LABELS))]\n",
        "        w.writerow(row)\n",
        "print(\"Saved:\", CSV_PATH)\n"
      ],
      "metadata": {
        "id": "RTVMMdHWhJrH",
        "outputId": "31aedf8a-b342-4a89-9b71-1a6b3cad9dc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: out_conch_style/patch_embeddings.npz | N = 166 | embs shape: (166, 512)\n",
            "Saved: out_conch_style/patch_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def draw_heatmap_rects(items,\n",
        "                       values: np.ndarray,   # (N,)\n",
        "                       out_png: str,\n",
        "                       alpha: float = 0.7):\n",
        "    \"\"\"\n",
        "    最小可行热图：每个 patch 画成一个矩形，强度=values（归一化到0~255）。\n",
        "    白底 + 红色通道叠加效果；若想更平滑可改为高斯权重/透明度叠加。\n",
        "    \"\"\"\n",
        "    assert len(items) == len(values)\n",
        "\n",
        "    # 计算画布大小（20×坐标系）\n",
        "    W = 0; H = 0\n",
        "    for it in items:\n",
        "        W = max(W, it[\"x20\"] + it[\"ps20\"])\n",
        "        H = max(H, it[\"y20\"] + it[\"ps20\"])\n",
        "\n",
        "    base = Image.new(\"RGB\", (W, H), (255, 255, 255))\n",
        "    overlay = Image.new(\"L\", (W, H), 0)\n",
        "    draw = ImageDraw.Draw(overlay)\n",
        "\n",
        "    v = values.astype(np.float32)\n",
        "    v = (v - v.min()) / (v.max() - v.min() + 1e-8)\n",
        "    v = (v * 255.0).astype(np.uint8)\n",
        "\n",
        "    for it, val in zip(items, v):\n",
        "        x, y, ps = it[\"x20\"], it[\"y20\"], it[\"ps20\"]\n",
        "        draw.rectangle([x, y, x + ps, y + ps], fill=int(val))\n",
        "\n",
        "    # 伪彩：红通道=强度；与白底 alpha 混合\n",
        "    heat = Image.merge(\"RGB\", (overlay, Image.new(\"L\", (W, H), 0), Image.new(\"L\", (W, H), 0)))\n",
        "    out = Image.blend(base, heat, alpha=alpha)\n",
        "    out.save(out_png)\n",
        "    return out_png\n"
      ],
      "metadata": {
        "id": "-kpaFo2kgAqT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最大相似度热图\n",
        "OUT_MAX = os.path.join(OUT_PREFIX, \"heatmap_maxsim_20x.png\")\n",
        "draw_heatmap_rects(items, sims.max(axis=1), OUT_MAX, alpha=0.7)\n",
        "print(\"Saved:\", OUT_MAX)\n",
        "\n",
        "# 各类别热图\n",
        "for j, lbl in enumerate(LABELS):\n",
        "    OUT_LBL = os.path.join(OUT_PREFIX, f\"heatmap_{lbl.replace(' ', '_')}_20x.png\")\n",
        "    draw_heatmap_rects(items, sims[:, j], OUT_LBL, alpha=0.7)\n",
        "    print(\"Saved:\", OUT_LBL)\n"
      ],
      "metadata": {
        "id": "fekkNjW8hqA_",
        "outputId": "af2f8e2d-44b0-4a3f-b7cb-11121612d6c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: out_conch_style/heatmap_maxsim_20x.png\n",
            "Saved: out_conch_style/heatmap_benign_gland_20x.png\n",
            "Saved: out_conch_style/heatmap_Gleason_pattern_3_20x.png\n",
            "Saved: out_conch_style/heatmap_Gleason_pattern_4_20x.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViD9Rxzuhr4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10c29447e5b54c9f94c5bac7e385c4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2693390143be4362a823cb9a6779137a",
              "IPY_MODEL_5e8a2fda75e44264bd9b87307c0a1214",
              "IPY_MODEL_adc9e5211bde4ee386d2351b5ba8d364"
            ],
            "layout": "IPY_MODEL_f0d2f158e885474c9d414edf8fc25ee4"
          }
        },
        "2693390143be4362a823cb9a6779137a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7cf18eaa92f4eaa902c95c79a1a1c26",
            "placeholder": "​",
            "style": "IPY_MODEL_44b88f14272141f284f5ac1906de8d29",
            "value": "open_clip_model.safetensors: 100%"
          }
        },
        "5e8a2fda75e44264bd9b87307c0a1214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26040edd1b2b4bd4a2f9e463ad47a767",
            "max": 605143284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d8bf235dca3435b8c55749503f66146",
            "value": 605143284
          }
        },
        "adc9e5211bde4ee386d2351b5ba8d364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3615fd32a65472ba0091dd4304f8a93",
            "placeholder": "​",
            "style": "IPY_MODEL_49682ea8502f4b10ae7b64be4fff3f44",
            "value": " 605M/605M [00:07&lt;00:00, 95.2MB/s]"
          }
        },
        "f0d2f158e885474c9d414edf8fc25ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7cf18eaa92f4eaa902c95c79a1a1c26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44b88f14272141f284f5ac1906de8d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26040edd1b2b4bd4a2f9e463ad47a767": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d8bf235dca3435b8c55749503f66146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3615fd32a65472ba0091dd4304f8a93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49682ea8502f4b10ae7b64be4fff3f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}